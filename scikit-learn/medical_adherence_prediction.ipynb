{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medication Compliance Forecasting \n",
    "\n",
    "### Objective \n",
    "Predict patientâ€™s likelihood of adherence to a prescribed regimen. For that,information is provided both for patient and the prescription. Patient information include details like age, gender, medical history, cultural background etc. Details about the prescription are like Diet control advised, Exercise advised etc.\n",
    "\n",
    "To build a predictive model, with the provided a data set that contains details about patient and the prescription.\n",
    "\n",
    "\n",
    "### Evaluation \n",
    "For each patient id in test set, you must predict if a patient is going to adhere to the prescribed regimen. Your model will be evaluated on precision and recall for both the outcomes. So, your code must include generation of confusion matrix for your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Setting up project directory and output paths, for data and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#To make outputs more consistent\n",
    "np.random.seed(42)\n",
    "\n",
    "#To Save & Load Models\n",
    "import pickle\n",
    "\n",
    "#To plot figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", color_codes = True)\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Preprocessing imports\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Models Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve , roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Folder Directory Structure\n",
    "PROJECT_ROOT_DIR ='.'\n",
    "PROJECT_DATA_DIR = './data'\n",
    "PROJECT_FOLDER = 'medical_compliance_forecasting'\n",
    "PROJECT_ID=PROJECT_FOLDER\n",
    "PROJECT_OUTPUT_PATH = os.path.join(PROJECT_ROOT_DIR,'model',PROJECT_ID)\n",
    "TRAINING_DATA =  os.path.join(PROJECT_DATA_DIR,PROJECT_FOLDER,'Training Data.csv')\n",
    "TEST_DATA =  os.path.join(PROJECT_DATA_DIR,PROJECT_FOLDER,'Test Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a : Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(fig_id,tight_layout=True,fig_extension='png',resolution=300):\n",
    "    if not os.path.exists(IMAGES_PATH):\n",
    "        os.makedirs(PROJECT_OUTPUT_PATH)\n",
    "    path = os.path.join(PROJECT_OUTPUT_PATH,fig_id + '.' + fig_extension)\n",
    "    print(\"Saving Figure : {}\".format(fig_id))\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path,format=fig_extension,dpi=resolution)\n",
    "\n",
    "def save_model(model,model_name):\n",
    "    model_file = os.path.join(PROJECT_OUTPUT_PATH,model_name+'.pkl')\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "def load_model(model,model_name):\n",
    "    model_file = os.path.join(PROJECT_OUTPUT_PATH,model_name+'.pkl')\n",
    "    with open(model_file, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def load_data(path=TRAINING_DATA):\n",
    "    data_file = os.path.join(path)\n",
    "    return pd.read_csv(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b : Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Used to See Correleation in Data against a selected Feature\n",
    "def get_correlation(corr_matrix,data,feature):\n",
    "    return corr_matrix[feature].sort_values(ascending=False)\n",
    "\n",
    "# To compute & display Precision,Recall & F1 Score\n",
    "\n",
    "def compute_scores(y_label,y_predicted):\n",
    "    print(\"Classsification Report - \")\n",
    "    print(classification_report(y_label, y_predicted))\n",
    "    cnf_matrix = confusion_matrix(y_label, y_predicted)\n",
    "    plot_confusion_matrix(cnf_matrix)\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(processed_training_label, predicted_label)\n",
    "    fpr, tpr, thresholds = roc_curve(processed_training_label, predicted_label)\n",
    "    logit_roc_auc = roc_auc_score(processed_training_label, predicted_label)\n",
    "    \n",
    "    print(\"Plots for Precision Recall & ROC can be visualised below\")\n",
    "    plot_precision_vs_recall(precisions, recalls)\n",
    "    plot_roc_curve(fpr, tpr,logit_roc_auc, label=None)\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cnf_matrix):\n",
    "    plt.clf()\n",
    "    plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "    classNames = ['Negative','Positive']\n",
    "    plt.title('Confusion Matrix for Medical Compliance')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=45)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cnf_matrix[i][j]))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1,])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr,logit_roc_auc, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title('Receiver operating characteristic',fontsize=14)\n",
    "    plt.text(0.5,0.3,'Area under the Graph - %0.2f'%logit_roc_auc)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 \n",
    "Since we already have different Training & Test sets, we don't neeed to create any splits. Hence, directly loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  load_data(path=TRAINING_DATA)\n",
    "test_set = load_data(path=TEST_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Train Data\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Test Data\n",
    "test_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all description of the columns for their Data-types, counts & Nulls\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the distribution and attributes of the Age & Prescription Period which have a range of values\n",
    "dataset[['Age','Prescription_period']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we see Objects types / Categorical Values, let's see the count\n",
    "print(\"Gender Distribution - \")\n",
    "print(dataset['Gender'].value_counts())\n",
    "print()\n",
    "print(\"Adherence Distribution - \")\n",
    "print(dataset['Adherence'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference 1 -\n",
    "\n",
    "1. We can see the data has no null/missing values.\n",
    "2. We see that Gender & Adherence are categorical values\n",
    "3. We can also see that \"F\"emale patients data is more than \"M\"ale. Though the data is not imbalanced.\n",
    "4. We can also see that Adherence is dominated by \"N\"o but is not imbalanced.\n",
    "5. Since we have categorical values, we need to convert them.\n",
    "6. Since our data does not have any missing value, we don't require any imputer for filling nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Numerical & Categorical Categories\n",
    "num_attribs = list(dataset.drop(['Gender','Adherence'], axis=1))\n",
    "cat_attribs = ['Gender','Adherence']\n",
    "\n",
    "print(\"Numerical Categories in Source Data : {}\".format(num_attribs))\n",
    "print(\"Categorical Categories in Source Data : {}\".format(cat_attribs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Encoder for Categorical Values\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "gender = ordinal_encoder.fit_transform(dataset[['Gender']])\n",
    "adherence = ordinal_encoder.fit_transform(dataset[['Adherence']])\n",
    "\n",
    "#Creating a copy of the Orginial Dataset to update the values for Further analysis\n",
    "dataset_numerical = dataset.copy()\n",
    "dataset_numerical['Gender'] = gender\n",
    "dataset_numerical['Adherence'] = adherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "data_prepared = full_pipeline.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see the graphical distribution too as follows \n",
    "#Vertical Axis has number of instances\n",
    "#Horizontal Axis has values of the attributes\n",
    "dataset_numerical.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We also create the correlation between features -\n",
    "corr = dataset_numerical.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "sns.heatmap(corr, vmax=.9, linewidths=0.01,\n",
    "            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\n",
    "plt.title('Correlation between features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_correlation(dataset_numerical.corr(),dataset_numerical,'Adherence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference 2\n",
    "\n",
    "1. From the histogram distribution we can infer that all the fields have binary values.\n",
    "2. The Ordinal Encoder is able to encode the Categorical Categories\n",
    "3. The Correlation Heat-map shoes some features being related to each other.\n",
    "4. The correlation with the target field, 'Adherence' shows that Prescription_period & Age highly affect the compliance to the prescription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Data Pre-processing\n",
    "\n",
    "Since we can see that \"Prescription_period\" affect the target inversely, it is important for us to have a equal distribution of it , in the training & test dataset. Hence, we would go for a StratifiedShuffleSplit.\n",
    "\n",
    "Steps to be followed - \n",
    "\n",
    "1. Pipeline to process input data for training.\n",
    "2. Create Training-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Prescription_period\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(dataset, dataset[\"Prescription_period\"]):\n",
    "    strat_train_set = dataset.loc[train_index]\n",
    "    strat_test_set = dataset.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Top 10 values of the test split set\n",
    "print((strat_test_set[\"Prescription_period\"].value_counts() / len(strat_test_set))[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Top 10 values of the original DataSet\n",
    "print((dataset[\"Prescription_period\"].value_counts() / len(dataset))[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Features & Target for training\n",
    "\n",
    "training_data = strat_train_set.drop(['Adherence','patient_id'],axis=1)\n",
    "training_label = strat_train_set[['Adherence']].copy()\n",
    "\n",
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Features & Target for training\n",
    "\n",
    "test_data = strat_test_set.drop(['Adherence','patient_id'],axis=1)\n",
    "test_label = strat_test_set[['Adherence']].copy()\n",
    "\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Numerical & Categorical Categories\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "imputer_attribs = ['Diabetes','Alcoholism','HyperTension','Smokes','Tuberculosis','Sms_Reminder']\n",
    "num_attribs = ['Age','Prescription_period']\n",
    "cat_attribs = ['Gender',]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\"),imputer_attribs),\n",
    "        (\"num\", StandardScaler(), num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "processed_training_data = full_pipeline.fit_transform(training_data)\n",
    "processed_training_label = ordinal_encoder.fit_transform(training_label).reshape(-1,)\n",
    "\n",
    "processed_test_data = full_pipeline.fit_transform(test_data)\n",
    "processed_test_label = ordinal_encoder.fit_transform(test_label).reshape(-1,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo 1 : Logistic Regression \n",
    "F1 Score :  88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our algorithm\n",
    "log_reg = LogisticRegression(solver = 'lbfgs',random_state=42, n_jobs = -1)\n",
    "log_reg.fit(processed_training_data, processed_training_label)\n",
    "#log_reg.predict(processed_training_data[11,:].reshape(1,-1))\n",
    "predicted_label = cross_val_predict(log_reg, processed_training_data, processed_training_label, cv=3)\n",
    "compute_scores(processed_training_label, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Algo 2 : Decision Tree \n",
    "F1 Score :  90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our algorithm\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42 )\n",
    "\n",
    "tree_clf.fit(processed_training_data, processed_training_label)\n",
    "#log_reg.predict(processed_training_data[11,:].reshape(1,-1))\n",
    "predicted_label = cross_val_predict(tree_clf, processed_training_data, processed_training_label, cv=3)\n",
    "compute_scores(processed_training_label, predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo 3 : Randome Forest Tree \n",
    "F1 Score :  9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our algorithm\n",
    "\n",
    "forest_clf  = RandomForestClassifier(n_estimators=500, random_state=42 , n_jobs = -1,\n",
    "                                     max_depth=2)\n",
    "                                   \n",
    "#forest_clf.fit(processed_training_data, processed_training_label)\n",
    "#log_reg.predict(processed_training_data[11,:].reshape(1,-1))\n",
    "predicted_label = cross_val_predict(forest_clf, processed_training_data, processed_training_label, cv=3)\n",
    "compute_scores(processed_training_label, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our algorithm\n",
    "\n",
    "mlp_clf  = MLPClassifier(hidden_layer_sizes=(100,50,10) ,random_state=42 ,learning_rate ='adaptive',max_iter=500)\n",
    "                                   \n",
    "#forest_clf.fit(processed_training_data, processed_training_label)\n",
    "#log_reg.predict(processed_training_data[11,:].reshape(1,-1))\n",
    "predicted_label = cross_val_predict(mlp_clf, processed_training_data, processed_training_label, cv=3)\n",
    "compute_scores(processed_training_label, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "predicted_label = cross_val_predict(extra_trees_clf, processed_training_data, processed_training_label, cv=3)\n",
    "compute_scores(processed_training_label, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
